#!/usr/bin/env python3
"""
Dedicated script to generate all B2B Vault content for static website
"""
import os
import sys
from B2Bscraper import B2BVaultAgent

def main():
    print("ğŸš€ B2B Vault Content Generator")
    print("=" * 50)
    print("This script will:")
    print("1. Scrape ALL articles from B2B Vault")
    print("2. Process them with AI analysis")
    print("3. Generate a comprehensive website")
    print("4. Create PDF reports")
    print("5. Prepare everything for static hosting")
    print("=" * 50)
    
    # Ask for confirmation
    response = input("\nStart comprehensive scraping? This may take 30-60 minutes. (y/N): ")
    if response.lower() not in ['y', 'yes']:
        print("âŒ Scraping cancelled.")
        return
    
    print("\nğŸŒ Starting comprehensive B2B Vault scraping...")
    
    # Initialize scraper
    agent = B2BVaultAgent(max_workers=5)
    
    # Step 1: Collect ALL articles
    print(f"\nğŸ“‘ Step 1: Collecting articles from ALL categories...")
    all_articles = agent.scrape_all_articles(preview=True)
    
    if not all_articles:
        print("âŒ No articles found")
        return
    
    print(f"\nğŸ“Š Collected {len(all_articles)} total articles")
    
    # Step 2: Process with AI
    print(f"\nğŸ¤– Step 2: Processing articles with AI analysis...")
    processed_articles = agent.process_multiple_articles_parallel(all_articles, preview=True)
    
    if not processed_articles:
        print("âŒ No articles were successfully processed")
        return
    
    print(f"\nâœ… Successfully processed {len(processed_articles)} articles")
    
    # Step 3: Generate outputs
    print(f"\nğŸ“„ Step 3: Generating PDF report...")
    pdf_path = agent.generate_comprehensive_pdf_report(processed_articles, preview=True)
    
    print(f"\nğŸŒ Step 4: Generating website...")
    website_path = agent.generate_advanced_website(processed_articles, pdf_path, preview=True)
    
    print(f"\nğŸ‰ CONTENT GENERATION COMPLETE!")
    print(f"ğŸ“Š Statistics:")
    print(f"   â€¢ Total articles scraped: {len(all_articles)}")
    print(f"   â€¢ Articles processed: {len(processed_articles)}")
    print(f"   â€¢ Categories: {len(set(a['tab'] for a in all_articles))}")
    print(f"   â€¢ Publishers: {len(set(a['publisher'] for a in all_articles))}")
    print(f"   â€¢ PDF report: {pdf_path}")
    print(f"   â€¢ Website: {website_path}")
    
    print(f"\nğŸ“‹ Next Steps:")
    print(f"1. Run: python3 prepare_netlify_deployment.py")
    print(f"2. Upload the netlify_site folder to your hosting platform")
    print(f"3. Your dashboard will be live with all the analyzed articles!")

if __name__ == "__main__":
    main()
