#!/usr/bin/env python3
"""
Memory-efficient version of B2B Vault scraper that processes articles in very small batches
"""
import os
import time
from B2Bscraper import B2BVaultAgent
import gc  # Garbage collection

def run_memory_efficient_scraping():
    """Run scraping with minimal memory usage"""
    print("ðŸš€ Starting MEMORY-EFFICIENT B2B Vault scraping")
    print("ðŸ’¾ This version uses minimal memory and processes articles slowly but safely")
    print("-" * 60)
    
    # Use very conservative settings
    agent = B2BVaultAgent(max_workers=1)  # Single worker only
    
    try:
        # Step 1: Collect articles from homepage (no processing yet)
        print("ðŸ“‘ Collecting articles from homepage...")
        all_articles = agent.scrape_all_articles_from_homepage(preview=True)
        
        if not all_articles:
            print("âŒ No articles found")
            return
        
        print(f"\nâœ… Found {len(all_articles)} total articles")
        print(f"ðŸ“‚ Categories: {len(set(a['tab'] for a in all_articles))}")
        print(f"ðŸ“° Publishers: {len(set(a['publisher'] for a in all_articles))}")
        
        # Step 2: Process in VERY small batches to avoid memory issues
        batch_size = 2  # Process only 2 articles at a time
        processed_articles = []
        
        total_batches = (len(all_articles) + batch_size - 1) // batch_size
        
        print(f"\nðŸ”„ Processing {len(all_articles)} articles in {total_batches} small batches...")
        print("ðŸ’¾ Using minimal memory - this will take longer but won't crash")
        
        for i in range(0, len(all_articles), batch_size):
            batch_num = (i // batch_size) + 1
            batch = all_articles[i:i + batch_size]
            
            print(f"\nðŸ“¦ Batch {batch_num}/{total_batches}: Processing {len(batch)} articles")
            
            try:
                # Process this small batch
                batch_processed = agent.process_multiple_articles(batch, preview=True)
                processed_articles.extend(batch_processed)
                
                print(f"   âœ… Batch {batch_num} complete: {len(batch_processed)} articles processed")
                print(f"   ðŸ“Š Total processed so far: {len(processed_articles)}")
                
                # Force garbage collection to free memory
                gc.collect()
                
                # Small delay between batches to be gentle on system
                time.sleep(3)
                
            except Exception as e:
                print(f"   âŒ Batch {batch_num} failed: {e}")
                print(f"   ðŸ”„ Continuing with next batch...")
                continue
        
        if not processed_articles:
            print("âŒ No articles were successfully processed")
            return
        
        print(f"\nðŸ“Š Processing complete: {len(processed_articles)} articles processed successfully")
        
        # Step 3: Generate outputs
        print(f"\nðŸ“„ Generating PDF report...")
        try:
            pdf_path = agent.generate_comprehensive_pdf_report(processed_articles, preview=True)
            print(f"âœ… PDF generated: {pdf_path}")
        except Exception as e:
            print(f"âŒ PDF generation failed: {e}")
            pdf_path = None
        
        print(f"\nðŸŒ Generating website...")
        try:
            website_path = agent.generate_website(processed_articles, pdf_path, preview=True)
            print(f"âœ… Website generated: {website_path}")
        except Exception as e:
            print(f"âŒ Website generation failed: {e}")
            website_path = None
        
        # Final summary
        print(f"\nðŸŽ‰ MEMORY-EFFICIENT SCRAPING COMPLETE!")
        print("=" * 60)
        print(f"ðŸ“° Total articles found: {len(all_articles)}")
        print(f"ðŸ¤– Articles processed: {len(processed_articles)}")
        print(f"ðŸ“‚ Categories: {len(set(a['tab'] for a in all_articles))}")
        print(f"ðŸ“° Publishers: {len(set(a['publisher'] for a in all_articles))}")
        if pdf_path:
            print(f"ðŸ“„ PDF: {pdf_path}")
        if website_path:
            print(f"ðŸŒ Website: {website_path}")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ Critical error: {e}")
        print("ðŸ’¡ Try reducing batch size further or check your system resources")

def run_test_scraping():
    """Run a test with just a few articles"""
    print("ðŸ§ª Running TEST scraping with limited articles...")
    
    agent = B2BVaultAgent(max_workers=1)
    
    try:
        # Collect articles
        all_articles = agent.scrape_all_articles_from_homepage(preview=True)
        
        if not all_articles:
            print("âŒ No articles found")
            return
        
        # Test with just 3 articles
        test_articles = all_articles[:3]
        print(f"\nðŸ§ª Testing with {len(test_articles)} articles...")
        
        processed_articles = agent.process_multiple_articles(test_articles, preview=True)
        
        if processed_articles:
            print(f"\nâœ… Test successful! Processed {len(processed_articles)} articles")
            print("ðŸ’¡ You can now run the full memory-efficient scraper")
            
            # Generate test outputs
            pdf_path = agent.generate_comprehensive_pdf_report(processed_articles, preview=True)
            website_path = agent.generate_website(processed_articles, pdf_path, preview=True)
            
            print(f"ðŸ“„ Test PDF: {pdf_path}")
            print(f"ðŸŒ Test Website: {website_path}")
        else:
            print("âŒ Test failed - no articles processed")
            
    except Exception as e:
        print(f"âŒ Test failed: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        run_test_scraping()
    else:
        run_memory_efficient_scraping()
